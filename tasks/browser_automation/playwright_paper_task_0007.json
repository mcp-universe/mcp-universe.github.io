{
    "category": "general",
    "question": "I'm new to the field of Large Multimodal Models (LMMs) and trying to get up to speed. I've heard that the BLIP series - BLIP, BLIP-2, and InstructBLIP - are foundational works I should understand. Could you help me find the arXiv IDs for these three papers from https://arxiv.org/? Remember to close the browser when you finish the task.",
    "output_format": {
        "Name_of_the_paper_1": "2xxx.xxxxx",
        "Name_of_the_paper_2": "2xxx.xxxxx",
        "Name_of_the_paper_3": "2xxx.xxxxx"
    },
    "use_specified_server": true,
    "mcp_servers": [
        {
            "name": "playwright"
        },
        {
            "name": "date"
        }
    ],
    "evaluators": [
        {
            "func": "json",
            "op": "playwright.is_dict_equal",
            "value": {
                "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation": "2201.12086",
                "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models": "2301.12597",
                "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning": "2305.06500"
            }
        }
    ]
} 